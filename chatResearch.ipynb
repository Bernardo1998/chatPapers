{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16038,
     "status": "ok",
     "timestamp": 1689526579708,
     "user": {
      "displayName": "XIAOFENG LIN",
      "userId": "15656402004234789839"
     },
     "user_tz": 420
    },
    "id": "tM9aji3ogv8b",
    "outputId": "3d1f8ad9-7d19-4885-8666-9bd6df509894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "# This notebook is designed for Google colab.\n",
    "# Modify the path for your local environments.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "\n",
    "import os\n",
    "\n",
    "# Change to your working dir\n",
    "os.chdir(\"/content/drive/MyDrive/chatResearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16178,
     "status": "ok",
     "timestamp": 1689526795290,
     "user": {
      "displayName": "XIAOFENG LIN",
      "userId": "15656402004234789839"
     },
     "user_tz": 420
    },
    "id": "OWA5nDrzkwbc",
    "outputId": "6dc222f5-dc8c-4f07-a031-576ba63f1d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2 (from -r requirements.txt (line 1))\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six (from -r requirements.txt (line 2))\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting openai (from -r requirements.txt (line 3))\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Collecting scholarly (from -r requirements.txt (line 4))\n",
      "  Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
      "Collecting requests (from -r requirements.txt (line 5))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six->-r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->-r requirements.txt (line 2))\n",
      "  Downloading cryptography-41.0.2-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/xflin/.local/lib/python3.10/site-packages (from openai->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /home/xflin/.local/lib/python3.10/site-packages (from openai->-r requirements.txt (line 3)) (3.8.4)\n",
      "Collecting arrow (from scholarly->-r requirements.txt (line 4))\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Collecting beautifulsoup4 (from scholarly->-r requirements.txt (line 4))\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting bibtexparser (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading bibtexparser-1.4.0.tar.gz (51 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting deprecated (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting fake-useragent (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting free-proxy (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading free_proxy-1.1.1.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httpx (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting selenium (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sphinx-rtd-theme (from scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/xflin/.local/lib/python3.10/site-packages (from scholarly->-r requirements.txt (line 4)) (4.5.0)\n",
      "Collecting idna<4,>=2.5 (from requests->-r requirements.txt (line 5))\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->-r requirements.txt (line 5))\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->-r requirements.txt (line 5))\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six->-r requirements.txt (line 2))\n",
      "  Using cached cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai->-r requirements.txt (line 3))\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/xflin/.local/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/xflin/.local/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/xflin/.local/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 3)) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/xflin/.local/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/xflin/.local/lib/python3.10/site-packages (from aiohttp->openai->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/xflin/.local/lib/python3.10/site-packages (from arrow->scholarly->-r requirements.txt (line 4)) (2.8.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Collecting pyparsing>=2.0.3 (from bibtexparser->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lxml (from free-proxy->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from httpx->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests->-r requirements.txt (line 5))\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17 (from selenium->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
      "Collecting sphinx<7,>=1.6 (from sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docutils<0.19 (from sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.0/570.0 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->-r requirements.txt (line 2))\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore<0.18.0,>=0.15.0->httpx->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5.0,>=3.0 (from httpcore<0.18.0,>=0.15.0->httpx->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7.0->arrow->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-devhelp (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-jsmath (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-serializinghtml>=1.1.5 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-qthelp (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Jinja2>=3.0 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting Pygments>=2.13 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting snowballstemmer>=2.0 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting babel>=2.9 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "Collecting alabaster<0.8,>=0.7 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting packaging>=21.0 (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly->-r requirements.txt (line 4))\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.0->sphinx<7,>=1.6->sphinx-rtd-theme->scholarly->-r requirements.txt (line 4))\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Building wheels for collected packages: bibtexparser, free-proxy\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.0-py3-none-any.whl size=42429 sha256=1fec4e82414c1b1c6e7ab6117d19b87db38516092b29deb171c246e7d80ec801\n",
      "  Stored in directory: /home/xflin/.cache/pip/wheels/5d/de/d5/4e42fd75d48106e7c4023d6594c2992db38afa7019f96139a2\n",
      "  Building wheel for free-proxy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for free-proxy: filename=free_proxy-1.1.1-py3-none-any.whl size=5637 sha256=426427a22c10e5b23ef7b686126c61d2195cd1a87980012bd25c25f487cb8d5c\n",
      "  Stored in directory: /home/xflin/.cache/pip/wheels/5a/96/c7/5a434714fff4fea9a59075428b142626e0a74f8c3bf90a50d0\n",
      "Successfully built bibtexparser free-proxy\n",
      "Installing collected packages: sortedcontainers, snowballstemmer, fake-useragent, wrapt, urllib3, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, soupsieve, sniffio, six, python-dotenv, PySocks, PyPDF2, pyparsing, Pygments, pycparser, packaging, MarkupSafe, lxml, imagesize, idna, h11, exceptiongroup, docutils, charset-normalizer, certifi, babel, attrs, alabaster, wsproto, requests, outcome, Jinja2, deprecated, cffi, bibtexparser, beautifulsoup4, anyio, trio, sphinx, httpcore, free-proxy, cryptography, arrow, trio-websocket, sphinxcontrib-jquery, pdfminer.six, openai, httpx, sphinx-rtd-theme, selenium, scholarly\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.3 PyPDF2-3.0.1 PySocks-1.7.1 Pygments-2.15.1 alabaster-0.7.13 anyio-3.7.1 arrow-1.2.3 attrs-23.1.0 babel-2.12.1 beautifulsoup4-4.12.2 bibtexparser-1.4.0 certifi-2023.5.7 cffi-1.15.1 charset-normalizer-3.2.0 cryptography-41.0.2 deprecated-1.2.14 docutils-0.18.1 exceptiongroup-1.1.2 fake-useragent-1.1.3 free-proxy-1.1.1 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 idna-3.4 imagesize-1.4.1 lxml-4.9.3 openai-0.27.8 outcome-1.2.0 packaging-23.1 pdfminer.six-20221105 pycparser-2.21 pyparsing-3.1.0 python-dotenv-1.0.0 requests-2.31.0 scholarly-1.7.11 selenium-4.10.0 six-1.16.0 sniffio-1.3.0 snowballstemmer-2.2.0 sortedcontainers-2.4.0 soupsieve-2.4.1 sphinx-6.2.1 sphinx-rtd-theme-1.2.2 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 trio-0.22.2 trio-websocket-0.10.3 urllib3-2.0.3 wrapt-1.15.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2013,
     "status": "ok",
     "timestamp": 1689529659079,
     "user": {
      "displayName": "XIAOFENG LIN",
      "userId": "15656402004234789839"
     },
     "user_tz": 420
    },
    "id": "-BlN4gDPz-ll",
    "outputId": "38df0d4c-09f3-48cb-f885-f5d6c8fe4eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting proxy connection..\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/chatResearch/chatResearch.py\", line 270, in <module>\n",
      "    engine = ScholarlySearch()\n",
      "  File \"/content/drive/MyDrive/chatResearch/ScholarlySearch.py\", line 17, in __init__\n",
      "    self.reset_proxy()\n",
      "  File \"/content/drive/MyDrive/chatResearch/ScholarlySearch.py\", line 22, in reset_proxy\n",
      "    self.pg.FreeProxies()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scholarly/_proxy_generator.py\", line 550, in FreeProxies\n",
      "    proxy = self._proxy_gen(None)  # prime the generator\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scholarly/_proxy_generator.py\", line 522, in _fp_coroutine\n",
      "    proxy_works = self._check_proxy(proxies)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/scholarly/_proxy_generator.py\", line 141, in _check_proxy\n",
      "    resp = session.get(\"http://httpbin.org/ip\", timeout=self._TIMEOUT)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 542, in get\n",
      "    return self.request('GET', url, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 440, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 415, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 244, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1283, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1329, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 205, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Try loading local files\n",
    "!python main.py --action=\"interactive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466231,
     "status": "ok",
     "timestamp": 1685141500586,
     "user": {
      "displayName": "XIAOFENG LIN",
      "userId": "15656402004234789839"
     },
     "user_tz": 420
    },
    "id": "mqsnny7phFdu",
    "outputId": "869b4565-ea28-4def-fd04-6972d72c3d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1it [00:19, 19.69s/it]\n",
      "0it [00:10, ?it/s]\n",
      "0it [00:14, ?it/s]\n",
      "0it [00:13, ?it/s]\n",
      "0it [00:12, ?it/s]\n",
      "0it [00:13, ?it/s]\n",
      "0it [00:21, ?it/s]\n",
      "0it [00:10, ?it/s]\n",
      "0it [00:09, ?it/s]\n",
      "0it [00:10, ?it/s]\n",
      "0it [00:12, ?it/s]\n",
      "0it [00:16, ?it/s]\n",
      "total_token: 28235\n"
     ]
    }
   ],
   "source": [
    "!python chatResearch.py --file='paper_lists/paper_list.csv' --search_keywords=\"evaluating trustworthiness in synthetic data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141852,
     "status": "ok",
     "timestamp": 1687135261211,
     "user": {
      "displayName": "XIAOFENG LIN",
      "userId": "15656402004234789839"
     },
     "user_tz": 420
    },
    "id": "L3n5PEjYwiMW",
    "outputId": "1b89000c-7a51-4464-e686-72162a084b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting proxy connection..\n",
      "using large language models to model tabular data\n",
      "0 large language models, tabular data, machine learning\n",
      "Getting abstracts from scholar\n",
      "https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf\n",
      "https://arxiv.org/pdf/2210.06280\n",
      "https://arxiv.org/pdf/2301.13808\n",
      "https://ieeexplore.ieee.org/iel7/6287639/10005208/10121440.pdf\n",
      "https://arxiv.org/pdf/2110.05679\n",
      "5\n",
      "Downloading from https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf\n",
      "0it [00:06, ?it/s]\n",
      "Downloading from https://arxiv.org/pdf/2210.06280\n",
      "Exception No /Root object! - Is this really a PDF? when loading: https://arxiv.org/pdf/2210.06280\n",
      "Downloading from https://arxiv.org/pdf/2301.13808\n",
      "0it [00:06, ?it/s]\n",
      "Downloading from https://ieeexplore.ieee.org/iel7/6287639/10005208/10121440.pdf\n",
      "Exception No /Root object! - Is this really a PDF? when loading: https://ieeexplore.ieee.org/iel7/6287639/10005208/10121440.pdf\n",
      "Downloading from https://arxiv.org/pdf/2110.05679\n",
      "0it [00:05, ?it/s]\n",
      "1 \"transformer-based models for tabular data\"\n",
      "Getting abstracts from scholar\n",
      "https://arxiv.org/pdf/2206.01410\n",
      "1\n",
      "Downloading from https://arxiv.org/pdf/2206.01410\n",
      "0it [00:04, ?it/s]\n",
      "total_token: 7365\n"
     ]
    }
   ],
   "source": [
    "!python chatResearch.py --action=\"iterate\" --topic='using large language models to model tabular data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 112259,
     "status": "error",
     "timestamp": 1687133035847,
     "user": {
      "displayName": "XIAOFENG LIN",
      "userId": "15656402004234789839"
     },
     "user_tz": 420
    },
    "id": "WO9ExCT9v8R4",
    "outputId": "7342455f-7785-4578-ca29-eff0099dc462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informer: Beyond efficient transformer for long sequence time-series forecasting\n",
      "transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i)  a ProbSparse self-attention mechanism, which achieves O(L log L) in time  long time-series\n",
      "\n",
      "\n",
      "Probabilistic transformer for time series analysis\n",
      "SSMs based on transformer architectures for multivariate time series, which  on time series  forecasting and human motion prediction and demonstrate that our Probabilistic Transformer (\n",
      "\n",
      "\n",
      "Deep transformer models for time series forecasting: The influenza prevalence case\n",
      "In this work, we presented a Transformer-based approach to forecasting time series data.  Compared to other sequencealigned deep learning methods, our approach leverages self\n",
      "\n",
      "\n",
      "Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting\n",
      "In this paper, we propose to apply Transformer in time series forecasting. Our experiments  on both synthetic data and real datasets suggest that Transformer can capture long-term\n",
      "\n",
      "\n",
      "A transformer-based framework for multivariate time series representation learning\n",
      "time series representation learning based on the transformer  on several public multivariate  time series datasets from various  for multivariate time series regression and classification.\n",
      "\n",
      "\n",
      "Transformers in time series: A survey\n",
      "Transformers optimized for time series data. For applications, we summarize and analyze  Transformers for popular time series  For each time series Transformer, we analyze its insights,\n",
      "\n",
      "\n",
      "Adversarial sparse transformer for time series forecasting\n",
      "Transformer (AST) for multiple time series forecasting, which is a framework that combines a  modified Transformer and  The discriminator can regularize the modified Transformer at the\n",
      "\n",
      "\n",
      "Anomaly transformer: Time series anomaly detection with association discrepancy\n",
      "Go beyond previous methods, we introduce Transformers to the unsupervised time series  anomaly detection and propose the Anomaly Transformer for association learning. To\n",
      "\n",
      "\n",
      "Traffic transformer: Capturing the continuity and periodicity of time series for traffic forecasting\n",
      "temporal dependencies in Transformer, we have proposed seven different ways of modeling  the continuity and periodicity of time series. The time series segment method achieved the\n",
      "\n",
      "\n",
      "Learning graph structures with transformer for multivariate time-series anomaly detection in IoT\n",
      "time series [8] as a copious amount of IoT sensors in many real-life scenarios consecutively  generate substantial volumes of time-series  to form a multivariate time series. In this case,\n",
      "\n",
      "\n",
      "Transformer winding faults detection based on time series analysis\n",
      "for the assessment of the transformer maintenance status. In this study, the time series   eled using the time series analysis for faults detection of the transformer winding. Different\n",
      "\n",
      "\n",
      "Transformers in time-series analysis: a tutorial\n",
      "of transformers for time-series applications. We then discuss some of the most popular recent  time-series Transformer  Finally, we provide “best practices” for training transformers in\n",
      "\n",
      "\n",
      "Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting\n",
      "Multivariate time series (MTS) are time series with multiple dimensions, where each  dimension represents a specific univariate time series (eg a climate feature of weather). MTS\n",
      "\n",
      "\n",
      "A transformer self-attention model for time series forecasting\n",
      "Since contextual data is a complex example of time series data, the transformer network   time series. In this paper, the modified transformer model use in a variety of time series with\n",
      "\n",
      "\n",
      "NAST: non-autoregressive spatial-temporal transformer for time series forecasting\n",
      "a Non-Autoregressive Transformer architecture for time series forecasting, aiming at  overcoming the time delay and accumulative error issues in the canonical Transformer. Moreover,\n",
      "\n",
      "\n",
      "Tranad: Deep transformer networks for anomaly detection in multivariate time series data\n",
      "Time-series discord discovery is another recently proposed method for fault prediction [16,  37, 58, 59]. Time series discords refer to the most unusual time series  the same time series. A\n",
      "\n",
      "\n",
      "Time-series transformer generative adversarial networks\n",
      "specifically on time-series data and present a model that can generate synthetic time-series  which  A transformer-based framework for multivariate time series representation learning. In\n",
      "\n",
      "\n",
      "Tactis: Transformer-attentional copulas for time series\n",
      "As we later describe, transformers allow TACTiS to view time series as sets of tokens, among  which non-local dependencies can be learned regardless of considerations like alignment\n",
      "\n",
      "\n",
      "A time series transformer based method for the rotating machinery fault diagnosis\n",
      "the process object of the visual Transformer (ViT) adopted in  on the Transformer, we propose  the Time Series Transformer ( is characterized by time series tokenizer, Transformer layer,\n",
      "\n",
      "\n",
      "Temporal fusion transformers for interpretable multi-horizon time series forecasting\n",
      "future time steps, is a crucial problem within time series  dates), other exogenous time series  (eg historical customer foot  multi-horizon time series forecasting particularly challenging.\n",
      "\n",
      "\n",
      "Gated transformer networks for multivariate time series classification\n",
      "suitable for the multivariate time series classification task time series modeling. Our preliminary  results provide a strong baseline for the Transformer Networks on multivariate time series\n",
      "\n",
      "\n",
      "Variational transformer-based anomaly detection approach for multivariate time series\n",
      "potential of the Transformer model in the field of time-series data. Still detection algorithms  based on Transformer at present.  the Transformer model in the field of multivariate time-series\n",
      "\n",
      "\n",
      "TCCT: Tightly-coupled convolutional transformer on time series forecasting\n",
      "into combining Transformer and CNN tightly. Three classic CNN architectures after  transformation have been successfully applied to Transformer models on time series forecasting in\n",
      "\n",
      "\n",
      "Are transformers effective for time series forecasting?\n",
      "long-term forecasting accuracy of Transformer-based TSF  extraction capabilities of the  Transformer architecture. Instead, it  of Transformer-based solutions for other time series analysis\n",
      "\n",
      "\n",
      "Time-series anomaly detection with stacked Transformer representations and 1D convolutional network\n",
      "In this paper, we generated 20 time series with anomalies that follows the mechanism of  NeurIPS-TS. We set the length to 1500 and the anomaly ratio α from 0.05 to 0.2 by 0.05.\n",
      "\n",
      "\n",
      "TiSAT: time series anomaly transformer\n",
      "While anomaly detection in time series has been an  Self-attention based  approaches, such as transformers, have  transformer approach for anomaly detection in time\n",
      "\n",
      "\n",
      "Tts-gan: A transformer-based time-series generative adversarial network\n",
      "Therefore, to add positional encoding on time series inputs, we only need to divide the width  evenly into multiple pieces and keep the height of each piece unchanged. This process is\n",
      "\n",
      "\n",
      "Learning to exploit invariances in clinical time-series data using sequence transformer networks\n",
      "In contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable  architecture that learns to identify and account for invariances in clinical time-series data.\n",
      "\n",
      "\n",
      "SpringNet: Transformer and Spring DTW for time series forecasting\n",
      "model, while the LogSparse Transformer [1] is a recently proposed variation of the Transformer  architecture for time series forecasting; we denote it as “Transformer” in Tables 1 and 2.\n",
      "\n",
      "\n",
      "Tabular transformers for modeling multivariate time series\n",
      "Here we propose neural network models that represent tabular time series that can optionally  leverage their hierarchical structure. This results in two architectures for tabular time series\n",
      "\n",
      "\n",
      "Spatial-temporal convolutional transformer network for multivariate time series forecasting\n",
      "Transformer-based model for multivariate time series forecasting, called the spatial–temporal  convolutional Transformer  -art methods and is more robust to nonsmooth time series data.\n",
      "\n",
      "\n",
      "Cross reconstruction transformer for self-supervised time series representation learning\n",
      "for time series from a new perspective and propose Cross Reconstruction Transformer (CRT)  to solve the aforementioned problems in a unified way. CRT achieves time series\n",
      "\n",
      "\n",
      "Foreformer: an enhanced transformer-based framework for multivariate time series forecasting\n",
      "Transformer at the sequence level and make it learn a better representation for time series,   address long sequence time series forecasting (LSTF), which extends Transformer with KL-\n",
      "\n",
      "\n",
      "Simultaneous-fault diagnosis considering time series with a deep learning transformer architecture for air handling units\n",
      "employs transformer architecture is proposed to diagnose the simultaneous faults with  time-series  The transformer architecture adopts a novel multi-head attention mechanism without\n",
      "\n",
      "\n",
      "Learning to Rotate: Quaternion Transformer for Complicated Periodical Time Series Forecasting\n",
      "Time series forecasting is a critical and challenging problem in many real applications. Recently,  Transformer-based models prevail in time series  , some models introduce series decom\n",
      "\n",
      "\n",
      "Memory-based Transformer with shorter window and longer horizon for multivariate time series forecasting\n",
      "that Transformer has potential in capturing long-term dependencies. However, in the field of  time series forecasting, Transformer  However, it is hard to provide sufficient time series input\n",
      "\n",
      "\n",
      "Exploring the potential of time-series transformers for process modeling and control in chemical systems: an inevitable paradigm shift?\n",
      "transformer model and demonstrate its practical applicability for non-trivial multivariate  time-series  First, we developed a time-series transformer (TST) with multiple encoder blocks that\n",
      "\n",
      "\n",
      "Etsformer: Exponential smoothing transformers for time-series forecasting\n",
      "To address these limitations, we propose ETSformer, an effective and efficient Transformer  architecture for time-series forecasting, inspired by exponential smoothing methods (\n",
      "\n",
      "\n",
      "Self-supervised pretraining of transformers for satellite image time series classification\n",
      "pretraining scheme to initialize a transformer-based network by  time series of a pixel. The  main idea of our proposal is to leverage the inherent temporal structure of satellite time series\n",
      "\n",
      "\n",
      "Preformer: predictive transformer with multi-scale segment-wise correlations for long-term time series forecasting\n",
      "We utilize the series decomposition blocks to decompose the time series into trend and  seasonal components. Our proposed MSSC and PreMSSC are the key modules to model the\n",
      "\n",
      "\n",
      "InSAR time-series deformation forecasting surrounding Salt Lake using deep transformer models\n",
      "deformation prediction methods based on the transformer model to evaluate the expansion   InSAR time series deformation forecasting method based on the multivariate transformer\n",
      "\n",
      "\n",
      "Multi-modal information analysis for fault diagnosis with time-series data from power transformer\n",
      "sample with the consideration of the time sequence of the power transformer data. And, the  time series data contains more information about the power transformer fault. The important\n",
      "\n",
      "\n",
      "TARNet: Task-Aware Reconstruction for Time-Series Transformer\n",
      "Time series shapelets: a new primitive for data mining. In KDD. 947–956.  ROCKET:  exceptionally fast and accurate time series classification using random convolutional kernels\n",
      "\n",
      "\n",
      "Multivariate time series anomaly detection with adversarial transformer architecture in the Internet of Things\n",
      "transformer multidimensional time series anomaly detection model for the IoT multidimensional  time series  Two-stage adversarial training enables the transformer to amplify the\n",
      "\n",
      "\n",
      "Interpretable wind speed prediction with multivariate time series and temporal fusion transformers\n",
      "Transformer mainly includes an encoder and a decoder, where the encoder part takes the  historical data of the time series  on the parts of the time series historical values that are most\n",
      "\n",
      "\n",
      "CLformer: Locally grouped auto-correlation and convolutional transformer for long-term multivariate time series forecasting\n",
      "time series decomposition method in the Transformer framework to enable the model to  extract short- and long-term time  In this paper, we propose a Transformer-based model named\n",
      "\n",
      "\n",
      "Probabilistic decomposition transformer for time series forecasting\n",
      "• We propose an effective time series forecasting model called Probabilistic Decomposition  Transformer, the model combines the Transformer architecture and generative model based\n",
      "\n",
      "\n",
      "… settlements classification via a transformer-based spatial-temporal fusion network using multimodal remote sensing and time-series human activity data\n",
      "time-series TPD data can reflect the traces and patterns of human activities. Besides, through  the statistics and analysis, a noticeable difference in the TPD time series  time-series TPD\n",
      "\n",
      "\n",
      "Evaluation of the transformer architecture for univariate time series forecasting\n",
      "the suitability of Transformers for time series forecasting,  Transformer with different architecture  and hyper-parameter configurations over 12 datasets with more than 50,000 time series.\n",
      "\n",
      "\n",
      "A Fusion Transformer for Multivariable Time Series Forecasting: The Mooney Viscosity Prediction Case\n",
      "We develop a general transformer-based model for time series forecasting that is capable  of performing simultaneous feature analysis on different types of variables and accurately\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-80510bb54449>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print(type(element), element.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#for key, value in element.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/publication_parser.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 class_='gs_ico gs_ico_nav_next').parent['href']\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/publication_parser.py\u001b[0m in \u001b[0;36m_load_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# this is temporary until setup json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gs_r gs_or gs_scl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gsc_mpat_ttl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_navigator.py\u001b[0m in \u001b[0;36m_get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;34m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://scholar.google.com{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'\\xa0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_navigator.py\u001b[0m in \u001b[0;36m_get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpagerequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpremium\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# premium methods may contain sensitive information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Session proxy config is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \"\"\"\n\u001b[0;32m-> 1041\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m   1042\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         )\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    930\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    964\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionNotAvailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;31m# The ConnectionNotAvailable exception is a special case, that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http_proxy.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 }\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start_tls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                     \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/backends/sync.py\u001b[0m in \u001b[0;36mstart_tls\u001b[0;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 sock = ssl_context.wrap_socket(\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 )\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scholarly import ProxyGenerator\n",
    "\n",
    "# Set up a ProxyGenerator object to use free proxies\n",
    "# This needs to be done only once per session\n",
    "pg = ProxyGenerator()\n",
    "pg.FreeProxies()\n",
    "scholarly.use_proxy(pg)\n",
    "\n",
    "# Now search Google Scholar from behind a proxy\n",
    "search_query = scholarly.search_pubs('Transformer time series')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        element = next(search_query)\n",
    "        #print(type(element), element.keys())\n",
    "        #for key, value in element.items():\n",
    "        #  print(key, value)\n",
    "        print(element['bib']['title'])\n",
    "        print(element['bib']['abstract'])\n",
    "        print('\\n')\n",
    "        #scholarly.pprint(element)\n",
    "    except StopIteration:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIINyvJr92JdRzGFAelUz3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
